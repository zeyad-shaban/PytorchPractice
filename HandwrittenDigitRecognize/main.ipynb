{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and standard deviation\n",
    "\n",
    "# torch.mean(dataset[])\n",
    "mean = dataset.data.float().mean() / 255\n",
    "std = dataset.data.float().std() / 255\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = dataset[0][0].numel()\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 1024),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.LeakyReLU(), # hmm is using ReLU even good in this case as it acts exactly like a linear model but without the negative x sidee\n",
    "    nn.Linear(128, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0012, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0198, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lossFn(predicted, target_batch)\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for data_batch, target_batch in train_loader:\n",
    "        predicted = model(data_batch.view(-1, input_size))\n",
    "        loss = lossFn(predicted, target_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "]))\n",
    "\n",
    "idx = 123\n",
    "\n",
    "plt.imshow(test_dataset.data[idx])\n",
    "model(test_dataset.data[idx].float().view(1, -1)).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(98.1000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for data_batch, target_batch in test_loader:\n",
    "        predicted = model(data_batch.view(-1, input_size)).argmax(dim=1)\n",
    "        correct += (predicted == target_batch).sum()\n",
    "        total += predicted.shape[0]\n",
    "\n",
    "(correct / total) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
